---
title: "STAT 413/613 Homework: Tidy Text"
author: "Ifebunandu Jerome Okeke"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: no
    toc_depth: 4
    number_sections: yes
    theme: cerulean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align  = "center",
                      fig.height = 5, 
                      fig.width  = 6)
```

# Instructions {-}
1. Clone this homework repo to your homework directory as a new repo.
2. Rename the starter file under the analysis directory as `hw_01_yourname.Rmd` and use it for your solutions.   
3. Modify the "author" field in the YAML header.  
4. Stage and Commit R Markdown and HTML files (no PDF files).   
5. **Push both .Rmd and HTML files to GitHub**.   
- Make sure you have knitted to HTML prior to staging, committing, and pushing your final submission.  
6. **Commit each time you answer a part of question, e.g. 1.1**   
7. **Push to GitHub after each major question**   
8. When complete, submit a response in Canvas  

- Only include necessary code to answer the questions.
- Most of the functions you use should be from the tidyverse. 
- Unnecessary Base R or other packages not covered in class will result in point deductions.
- Use Pull requests and or email to ask me any questions. If you email, please ensure your most recent code is pushed to GitHub.


# Sentiment Analysis

1. Download the following two works from the early 20^th^ century from Project Gutenberg:
- Upton Sinclair: "*The Jungle*" (1906)
- W.E.B. Du Bois: "*The Quest of the Silver Fleece*" (1911)
  
```{r}
# Load library

library(tidyverse)
library(tidytext)
library(gutenbergr)

```
##
##
```{r}

Sinclair <- gutenberg_works(title == "The Jungle") %>%
  
  gutenberg_download()
 
```
##
##
```{r}

Duboise <- gutenberg_works(str_detect(title, "The Quest of the Silver Fleece")) %>%
  
  gutenberg_download()

```
##
##

2. Write a function `to take an argument of a downloaded book tibble and return it in tidy text format.
- The function must add line and chapter numbers as variables
- The function must unnest tokens at the word level
- The function must remove any Project Gutenberg formatting so only the words remain
- The function must remove any stop_words and filter out any `NA`s
- The function must remove any front matter (words before Chapter 1)
- The function can consider the unique nature of the front matter but cannot consider exactly how many chapters are in each book based on looking at the data i.e., no math based on knowing the number of chapters. 
   
##
##
```{r}

book_tidy <- function(x) {
  stopifnot(is.tibble(x),
            is.integer(x[[1]]),
            is.character(x[[2]]))
  if (x$text %>% has_element("_Contents_") == TRUE) {
    if (x$text %>% has_element("_Note_") == TRUE) {
    x %>% 
      filter(text != "_Contents_" & text != "_Note_") %>%
      mutate(linenumber = row_number(),
             chapter = cumsum(str_detect(text, 
                                         regex("^_[[:alpha:]]+_$|^_[[:alpha:]]+-[[:alpha:]]+_$",
                                               ignore_case = FALSE)))) %>% 
      unnest_tokens(word, text) %>% 
      mutate(word = str_extract(word, "[a-z']+")) %>% 
      anti_join(stop_words, by = "word") %>% 
      filter(chapter > 0,
             !is.na(word)) 
    }
  } 
  else {
    x %>% 
      mutate(linenumber = row_number(),
             chapter = cumsum(str_detect(text, 
                                         regex("^chapter [\\divxlc]",
                                               ignore_case = TRUE)))) %>% 
      unnest_tokens(word, text) %>% 
      mutate(word = str_extract(word, "[a-z']+")) %>% 
      anti_join(stop_words, by = "word") %>% 
      filter(chapter > 0,
             !is.na(word))
  }
}

book_tidy(Sinclair)

book_tidy(Duboise)


```
##
##
3. Use the function from step 2
- Tidy each book and then add `book` and `author` as variables and save each tibble to a new variable. How many rows are in each book?

```{r}
book_tidy(Sinclair) -> sinclair

book_tidy(Duboise) -> Duboise

```
###
###
###
```{r}

sinclair_new <- sinclair %>% 
  mutate(book = gutenberg_works(title == "The Jungle")[[2]],
         author = gutenberg_works(title == "The Jungle")[[3]]) 

 sinclair_new 

```
**-** There are 47,828 rows in Sinclair book
##
##
```{r}

Duboise_new <- Duboise %>% 
  mutate(book = gutenberg_works(title == "The Quest of the Silver Fleece: A Novel")[[2]],
         author = gutenberg_works(title == "The Quest of the Silver Fleece: A Novel")[[3]])

Duboise_new

```
**-** There are 41,606 rows in Dubois book.
## 
###
4. Use a dplyr function to combine the two tibbles into a new tibble. 
- It should have 89,434 rows with 6 variables

```{r}

comb_sinclair_Dubiose <- sinclair_new %>% 
  full_join(Duboise_new, by = c("gutenberg_id", "linenumber", "chapter", "word", "book", "author"))

comb_sinclair_Dubiose


```

5. Measure the net sentiment using bing for each block of 50 lines
- Plot the sentiment for each book in an appropriate faceted plot - either line or column. 
- Be sure to remove the legend.
- Save the plot to a variable
- Interpret the plots for each book and compare them.

```{r}

comb_sinclair_Dubiose %>%
  inner_join(get_sentiments("bing"), by = "word") %>% 
  count(book, index = linenumber %/% 50, sentiment) %>% 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = list(n=0)) %>% 
  mutate(net = positive - negative) ->
  comb_sinclair_Dubiose_sentiment

comb_sinclair_Dubiose_sentiment %>%
  ggplot(aes(index, net, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")

```
**-** Looking at the plot, it reveals that the comment on both books are largely negative.
**-** The Quest of the silver fleece has more positive comments than the jungle 
**-** Both books have a large negative comments than positive with jungle exhibiting a slightly higher negative comment than the Quest of the silver fleece

6. Measure the total for each nrc sentiment in each block of 500 lines and then,
- Filter out the "positive" and "negative" and save to a new variable. You should have 464 observations.
- Plot the count of the sentiments for each block in each book in an appropriate faceted plot with the books in two columns and the sentiments in 8 rows. 
- Be sure to remove the legend.
- Interpret the plots for each book and then compare them. 
- Why did the values drop off so suddenly at the end?

##

```{r}
comb_sinclair_Dubiose %>%
  inner_join(get_sentiments("nrc"), by = "word") %>% 
  count(book, index = linenumber %/% 500, sentiment) %>%
  filter(sentiment != "positive",
         sentiment != "negative") ->
  
  sinclair_Dubiose_nrc



```


##

```{r}

sinclair_Dubiose_nrc %>%
  ggplot(aes(x = index, y = n, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_grid(rows = vars(sentiment), cols = vars(book), scales = "free_x") + 
  labs(y = "Contribution to sentiment", x = NULL)
  
```
**-** The plot reveals that the comments of words from the Jungle starts and drops off at the end. 
While comments of words from the quest of the silver fleece mostly starts and ends on higher comments.
Trust appears to have more comments in both plots but surprise appears to generate less comments on both plots.
The values dropping off suddenly may signify that the readers are neutral about about the comments on their overall feelings of the Jungle book.


7. Using bing, create a new data frame with the counts of the positive and negative sentiment words for each book.
- Show the "top 20" most frequent words across both books along with their book, sentiment, and count, in descending order by count.
- What are the positive words in the list of "top 20"?

```{r}

comb_sinclair_Dubiose %>%
  group_by(book) %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = TRUE) %>%
  arrange() %>%
  ungroup() %>%
  head(20) -> comb_sclr_Dub_top_20

comb_sclr_Dub_top_20 %>%
  
  filter(sentiment == "positive") 
  
  comb_sclr_Dub_top_20


```
##
##
8. Plot the top ten for each positive and negative sentiment faceting by book.
- Ensure each facet has the words in the proper order for that book.
- Identify any that may be inappropriate for the context of the book and should be excluded from the sentiment analysis.

```{r}

comb_sinclair_Dubiose %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(book, word, sentiment, sort = TRUE) %>%
  group_by(sentiment) %>%
  slice_max(order_by = n, n = 10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) + 
  facet_grid(book ~ sentiment, scales = "free_y") +
  labs(y = "contribution of sentiment", x = NULL) +
  coord_flip()
  
```
**-** "miss" seems to be inappropriate for the context of the book and should be excluded from the sentiment analysis

9. Remove the inappropriate word(s) from the analysis.
- Replot the top 10 for each sentiment per book from step 8.
- Interpret the plots

```{r}
get_sentiments("bing") %>%
  filter(word != "miss") ->
bing_no_miss

```
##
##
```{r}
comb_sinclair_Dubiose %>%
  inner_join(bing_no_miss, by = "word") %>%
  count(book, word, sentiment, sort = TRUE) %>%
  group_by(sentiment) %>%
  slice_max(order_by = n, n = 10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) + 
  facet_grid(book ~ sentiment, scales = "free_y") +
  labs(y = "contribution of sentiment", x = NULL) +
  coord_flip()


```
**-** The jungle has more words with negative comments in the top half of the book and a significant less amount of positive words in the book. 
**-** On then other hand, the Quest of the the silver fleece has few but significant amount of negative comments on the words used but also have higher number of positive comments in the first half of the book. 

##
##

10. Rerun the analysis from step 5 and recreate the plot with the title "Custom Bing".
- Show both the original step 5 plot with the new plot in the same output graphic, one on top of the other.
- Interpret the plots

##
##

```{r}
# Original

comb_sinclair_Dubiose %>%
  inner_join(get_sentiments("bing"), by = "word") %>% 
  count(book, index = linenumber %/% 50, sentiment) %>% 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = list(n=0)) %>% 
  mutate(net = positive - negative) ->
  comb_sinclair_Dubiose_sentiment
  

comb_sinclair_Dubiose %>%
  inner_join(bing_no_miss, by = "word") %>% 
  count(book, index = linenumber %/% 50, sentiment) %>% 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = list(n=0)) %>% 
  mutate(net = positive - negative) ->
  comb_sinclair_Dubiose_sentiment2 



comb_sinclair_Dubiose_sentiment %>%
  ggplot(aes(index, net, fill = book)) +
  geom_col(show.legend = FALSE) +
  ggtitle("With Miss as Negative") +
  facet_wrap(~book, ncol = 2, scales = "free_x") ->
  
  p1


comb_sinclair_Dubiose_sentiment2 %>%
  ggplot(aes(index, net, fill = book)) +
  geom_col(show.legend = FALSE) +
  ggtitle("Custom Bing") +
  facet_wrap(~book, ncol = 2, scales = "free_x") ->
    
    p2

library(gridExtra)
  
grid.arrange(p1, p2, nrow =2)  
  

```
**-** The plots are almost identical. The first plots with "miss" reveals large negative comments on both plots and a higher positive comments in the Quest of the silver fleece novel than the Jungle novel. However, when "miss" was removed, It didn't have much of an effect in the jungle plot because the plot stayed almost the same. But there is a slight change in the positive comments on the Quest of the silver fleece plot especially around the 0-100 range.

##
##

# tf-idf for Mark Twain's books

1. Use a single call to download all the following complete books at once from author Mark Twain from Project Gutenberg
- Use the meta_fields argument to include the Book title as part of the download
- *Huckleberry Finn*,  *Tom Sawyer* , *Connecticut Yankee in King Arthur's Court*, *Life on the Mississippi* , *Prince and the Pauper*,  and *A Tramp Abroad* 


```{r}

gutenberg_works() %>% 
  filter(author %in% "Twain, Mark",
         title %in% c("A Tramp Abroad", 
                      "A Connecticut Yankee in King Arthur's Court",
                      "The Prince and the Pauper",
                      "Life on the Mississippi",
                      "Adventures of Huckleberry Finn",
                      "The Adventures of Tom Sawyer")) %>% 
  gutenberg_download(meta_fields = "title") -> mark_twin_books


mark_twin_books


```

2. Modify your earlier function or create a new one to output a tf-idf ready dataframe (**leave the the stop words in the text**)
- Unnest, remove any formatting, and get rid of any `NA`s  
- Add the count for each word by title.
- Use your function to tidy the downloaded texts and save to a variable. It should have 56,759 rows.


```{r}

book_tidy_2 <- function(x) {
  stopifnot(is.tibble(x),
            is.integer(x[[1]]),
            is.character(x[[2]]),
            is.character(x[[3]]))
  x %>% 
  unnest_tokens(word, text) %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(title, word, sort = TRUE) %>% 
  filter(!is.na(word))
}

book_tidy_2(mark_twin_books) -> new_book_tidy

new_book_tidy


```

3. Calculate the tf-idf
- Save back to the data frame.

```{r}


new_book_tidy %>%
  
  bind_tf_idf(word, title,n) -> new_book_tidy

new_book_tidy

```

4. Plot the tf for each book using a faceted graph.
- Facet by book and constrain the data or the X axis to see the shape of the distribution.

```{r}

new_book_tidy %>% 
  ggplot(aes(x = tf, fill = title)) +
  geom_histogram(show.legend = FALSE, bins = 50) +
  xlim(NA, 0.0005) +
  facet_wrap(~title, ncol = 2, scales = "free_y")


```

   
5. Show the words with the 15 highest tf-idfs across across all books
- Only show those rows.
- How many look like possible names?

```{r}

new_book_tidy %>% 
  top_n(15, tf_idf) %>%
  mutate(word = fct_reorder(word, tf_idf)) %>% 
  ggplot(aes(x = word, y = tf_idf, fill = title)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf_idf") +
  facet_wrap(~title, ncol = 2, scales = "free") +
  coord_flip()

```
**-**The Adventure of Tom Sawyer and the Prince and the Pauper names look like possible names
with names like becky, huck, joe, don, sid, hendon, canty, prince.

6.  Plot the top 7 tf_idf words from each book.
- Sort in descending order of tf_idf
- Interpret the plots.

```{r}

new_book_tidy %>%
  group_by(title) %>% 
  top_n(7, tf_idf) %>% 
  ungroup() %>% 
  mutate(word = fct_reorder(word, tf_idf)) %>% 
  ggplot(aes(x = word, y = tf_idf, fill = title)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf_idf") +
  facet_wrap(~title, ncol = 2, scales = "free") +
  coord_flip()

```
**-** The word with the most occurrence in A Connecticut Yankee in the King Arthur's Court is "launcelot" followed by Merlin.   
**-** The word with the most occurrence in A Tramp Road, is glacier followed by corps.
**-** The word with the most occurrence in Adventures of Huckleberry Finn is warn't followed by en.
**-** The word with the most occurrence in Adventure of Tom sawyer is becky followed by huck
**-** The word with the most occurrence in Life on the Mississippi is pilots followed by pilot
**-** The word with the most occurrence in The prince and the pauper is hendon followed by canty.

# Extra Credit Podcasts

- Choose **One** of the following podcasts and answer the questions below:  

a. [Sentiment Preserving Fake Reviews](https://podcasts.apple.com/us/podcast/data-skeptic/id890348705?i=1000483067378)  
The [Original paper](https://arxiv.org/abs/1907.09177)

b. [Data in  Life: Authorship Attribution in Lennon-McCartney Songs](https://podcasts.apple.com/us/podcast/authorship-attribution-of-lennon-mccartney-songs/id890348705?i=1000485519404)

## Ans: Sentiment Preserving Fake Reviews.

1. What are some key ideas from this podcast relevant to text sentiment analysis/authorship attribution?

**-** One of key ideas that is relevant to text sentiment analysis is the importance of building a model and  a system that helps in distinguishing between fake reviews and real reviews. It is increasingly more difficult to tell a fake reviews than tell a real review. From the statistics presented by the guest speaker, only 25% of their sample analysis was able to accurately identify  real reviews from the fake reviews. This is a big problem and suggest that the level of misinformation that people are exposed to.   

2. How do you think the ideas discussed may be relevant in your future work?
**-** The ideas will be useful in the sense that one has to properly analyze the content of the sentiments and their credibility of the user who provides this information and also have a system that guides user which which enable human detect fake reviews. It is also important to analyze story and not text. This also helps in identifying fake reviews.








